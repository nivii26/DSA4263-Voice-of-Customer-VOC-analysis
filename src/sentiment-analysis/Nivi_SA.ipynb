{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fddcd528-19d2-4d99-a21a-fe11142b8dee",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d3a2e25-61f5-477f-941a-8147d600bac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nnerella/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nnerella/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nnerella/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pre-processing Packages\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import nlpaug.augmenter.word as naw\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# download necessary NLTK data (only need to run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "994b4350-0542-4d5f-9798-c572dc3c8429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/nnerella/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# FOR VADER\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97da841-8247-4802-9947-be8cf8d5d113",
   "metadata": {},
   "source": [
    "## Import Data and Pre-Processing (DON'T RUN IT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd22e197-cda6-4a06-9381-561bd212441c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the data from the csv file\n",
    "train_data = pd.read_csv(\"../../data/raw/reviews.csv\")\n",
    "# train_label = train_data[\"Sentiment\"]\n",
    "original_data = pd.DataFrame(train_data[\"Text\"])\n",
    "\n",
    "# define a function to preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    # remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-z]', ' ', text)\n",
    "    # tokenize the text into words\n",
    "    tokens = word_tokenize(text)\n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# remove the html symbol\n",
    "def remove_html(text):\n",
    "    regex = r\"<[^>]+>\"\n",
    "    text_new = re.sub(regex, \" \", text)\n",
    "    return text_new\n",
    "\n",
    "# apply the preprocessing function to the text data\n",
    "train_data['Text'] = train_data['Text'].apply(remove_html)\n",
    "train_data['Text'] = train_data['Text'].apply(preprocess_text)\n",
    "\n",
    "# define an NLPAug data augmentation function\n",
    "def augment_text(text):\n",
    "    # define an augmentation method\n",
    "    aug = naw.SynonymAug(aug_src='wordnet', lang='eng')\n",
    "    # apply the augmentation method to the text\n",
    "    augmented_text = aug.augment(text)\n",
    "    return augmented_text\n",
    "\n",
    "# apply the augmentation function to the preprocessed text data\n",
    "train_data['Text'] = train_data['Text'].apply(augment_text)\n",
    "\n",
    "# over sampling\n",
    "ros = RandomOverSampler(sampling_strategy='minority')\n",
    "X = train_data['Text'].values.reshape(-1, 1)\n",
    "y = train_data['Sentiment']\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "train_data = pd.DataFrame({'Text': X_resampled.ravel(), 'Sentiment': y_resampled})\n",
    "\n",
    "# save the data to a new csv file\n",
    "train_data.to_csv(\"oversampling_reviews.csv\", index=False)\n",
    "\n",
    "## Features\n",
    "# train a Word2Vec model on the preprocessed text data\n",
    "word2vec_model = Word2Vec(train_data['Text'], min_count=1)\n",
    "\n",
    "# create a function to generate the word embedding vectors for each sentence\n",
    "def generate_word_embedding(sentence):\n",
    "    # initialize an empty array for the sentence vector\n",
    "    sentence_vector = []\n",
    "    # loop through each word in the sentence\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            # add the vector representation of the word to the sentence vector\n",
    "            word_vector = word2vec_model.wv[word]\n",
    "            sentence_vector.append(word_vector)\n",
    "        except KeyError:\n",
    "            # ignore words that are not in the vocabulary\n",
    "            pass\n",
    "    # take the mean of the word vectors to get the sentence vector\n",
    "    sentence_vector = np.mean(sentence_vector, axis=0)\n",
    "    return sentence_vector\n",
    "\n",
    "# apply the generate_word_embedding() function to the preprocessed text data\n",
    "train_data['embedding'] = train_data['Text'].apply(generate_word_embedding)\n",
    "\n",
    "# create a new DataFrame for the feature matrix\n",
    "embedding_size = word2vec_model.vector_size\n",
    "features_df = pd.DataFrame(train_data['embedding'].tolist(), columns=[f'embedding_{i}' for i in range(embedding_size)])\n",
    "\n",
    "# perform PCA with n_components set to retain 98% of variance\n",
    "pca_emb = PCA(n_components=0.98)\n",
    "features_emb_pca = pca_emb.fit_transform(features_df)\n",
    "\n",
    "# create a new DataFrame for the PCA features\n",
    "pca_emb_cols = [f\"PC_emb{i+1}\" for i in range(features_emb_pca.shape[1])]\n",
    "pca_df_emb = pd.DataFrame(features_emb_pca, columns=pca_emb_cols)\n",
    "\n",
    "\n",
    "# create a TF-IDF vectorizer object\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# fit and transform the vectorizer on the preprocessed text data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(train_data['Text'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_features_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_feature_names)\n",
    "## standardize the features\n",
    "#scaler = StandardScaler()\n",
    "#features_std = scaler.fit_transform(features)\n",
    "\n",
    "# perform PCA with n_components set to retain 95% of variance\n",
    "pca = PCA(n_components=0.95)\n",
    "features_tfidf_pca = pca.fit_transform(tfidf_features_df)\n",
    "\n",
    "# create a new DataFrame for the PCA features\n",
    "pca_tfidf_cols = [f\"PC_tfidf{i+1}\" for i in range(features_tfidf_pca.shape[1])]\n",
    "pca_df_tfidf = pd.DataFrame(features_tfidf_pca, columns=pca_tfidf_cols)\n",
    "\n",
    "# add the TF-IDF features to the feature matrix DataFrame\n",
    "features_df = pd.concat([pca_df_tfidf, pca_df_emb], axis=1)\n",
    "\n",
    "\n",
    "# add the number of characters, number of words, and number of capital characters as features\n",
    "features_df['num_characters'] = train_data['Text'].apply(lambda x: len(' '.join(x)))\n",
    "features_df['num_words'] = train_data['Text'].apply(lambda x: len(x))\n",
    "\n",
    "# add the common features from the features.csv file\n",
    "features_df['num_sentences'] = original_data[\"Text\"].apply(lambda s: s.count('.'))\n",
    "features_df['num_question_marks'] = original_data[\"Text\"].apply(lambda s: s.count('?'))\n",
    "features_df['num_exclamation_marks'] = original_data[\"Text\"].apply(lambda s: s.count('!'))\n",
    "features_df['num_unique_words'] = train_data[\"Text\"].apply(lambda x: len(set(x)))\n",
    "\n",
    "\n",
    "# add the label column to the feature matrix DataFrame\n",
    "label = features_df.columns\n",
    "features_df['Sentiment'] = train_data['Sentiment']\n",
    "\n",
    "# weight the negative sentiment samples by 1.5\n",
    "features_df.loc[features_df['Sentiment'] == 'negative',label] *= 2\n",
    "\n",
    "\n",
    "# save the feature matrix to a CSV file\n",
    "# pca_df_emb.to_csv(\"pca_df_emb.csv\", index=False)\n",
    "# pca_df_tfidf.to_csv(\"pca_df_tfidf.csv\", index=False)\n",
    "features_df.to_csv(\"features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219a9340-b2eb-4e3f-975c-18e606cb8806",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8eade87-1ed5-4fb8-91b0-e9e36ca7410f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Features obtained after over-sampling and PCA\n",
    "features_df = pd.read_csv(\"features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4920958b-2ac1-4eb8-9986-38129749e8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8060 entries, 0 to 8059\n",
      "Columns: 3340 entries, PC_tfidf1 to Sentiment\n",
      "dtypes: float64(3336), int64(3), object(1)\n",
      "memory usage: 205.4+ MB\n"
     ]
    }
   ],
   "source": [
    "features_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb415946-9ccd-450b-a8ee-76eeeeb80b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    'positive': 1,\n",
    "    'negative': 0,\n",
    "}\n",
    "\n",
    "features_df['sentiment_label'] = features_df['Sentiment'].map(label_map)\n",
    "\n",
    "# X\n",
    "train_cols = features_df.iloc[: , :-7]\n",
    "# y\n",
    "label_col = features_df['sentiment_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb5092bd-cac1-43bd-ad69-293154195ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_cols, label_col, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b0b605-25af-4132-ae04-2a90c0ac28a9",
   "metadata": {},
   "source": [
    "## Baseline: Naive Bayes' Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cba9cbc-682a-43b6-a34d-f3567a4111d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores are [0.99937965 0.99813896 0.98511166 0.99441687 1.        ]\n",
      "Average Cross Validation score :0.9954094292803969\n"
     ]
    }
   ],
   "source": [
    "NB = GaussianNB()\n",
    "kf = KFold(n_splits=5)\n",
    "score = cross_val_score(NB, train_cols, label_col,cv=kf)\n",
    "print(\"Cross Validation Scores are {}\".format(score))\n",
    "print(\"Average Cross Validation score :{}\".format(score.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2b46435-be04-4ec5-9379-1d022039fd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9971050454921423\n"
     ]
    }
   ],
   "source": [
    "NB.fit(X_train, y_train)\n",
    "NB_pred= NB.predict(X_test)\n",
    "#print(NB_pred)\n",
    "\n",
    "print(accuracy_score(y_test, NB_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05338a64-f6b2-4527-b66f-28bd487fbfff",
   "metadata": {},
   "source": [
    "## VADER(Valence Aware Dictionary for Sentiment Reasoning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdf4a6c-15af-440b-b1ab-c20563368745",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Links for Reference\n",
    "1. https://www.analyticsvidhya.com/blog/2022/10/sentiment-analysis-using-vader/#:~:text=VADER(%20Valence%20Aware%20Dictionary%20for,as%20either%20positive%20or%20negative.\n",
    "\n",
    "2. https://stackoverflow.com/questions/45296897/is-there-a-way-to-improve-performance-of-nltk-sentiment-vader-sentiment-analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8812cacb-688c-4e4e-991d-200c5aa1c29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_csv = pd.read_csv('../../data/raw/reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaf1b674-bc45-409a-900f-a7aff062b594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Sentiment into factor\n",
    "# Positive: 1, Negative: 0\n",
    "\n",
    "label_map = {\n",
    "    'positive': 1,\n",
    "    'negative': 0,\n",
    "}\n",
    "reviews_csv['sentiment_label'] = reviews_csv['Sentiment'].map(label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ff8af1-6efc-4b8a-8ea0-f95c3e3b3b7a",
   "metadata": {},
   "source": [
    "#### Note: \n",
    "1. nltk+vader already do basic pre-processing (such as removing stop-words etc..)\n",
    "2. Do not remove punctuation, as it helps improve score (Ex. ! -> +ve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "645377fc-b6f3-4d4e-acd4-3415bbe283b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HTML characters\n",
    "def remove_html(text):\n",
    "    regex = r\"<[^>]+>\"\n",
    "    text_new = re.sub(regex, \" \", text)\n",
    "    return text_new\n",
    "\n",
    "reviews_csv['VADER_processed_Text'] = reviews_csv['Text'].apply(lambda para: remove_html(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6448f942-5512-4d58-8766-06cc4e18c546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>VADER_processed_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>18/6/21</td>\n",
       "      <td>This is a very healthy dog food. Good for thei...</td>\n",
       "      <td>1</td>\n",
       "      <td>This is a very healthy dog food. Good for thei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>7/7/21</td>\n",
       "      <td>I've been very pleased with the Natural Balanc...</td>\n",
       "      <td>1</td>\n",
       "      <td>I've been very pleased with the Natural Balanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>18/6/21</td>\n",
       "      <td>Before I was educated about feline nutrition, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Before I was educated about feline nutrition, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>7/7/21</td>\n",
       "      <td>My holistic vet recommended this, along with a...</td>\n",
       "      <td>1</td>\n",
       "      <td>My holistic vet recommended this, along with a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>1/7/21</td>\n",
       "      <td>I bought this coffee because its much cheaper ...</td>\n",
       "      <td>1</td>\n",
       "      <td>I bought this coffee because its much cheaper ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5439</th>\n",
       "      <td>negative</td>\n",
       "      <td>26/2/21</td>\n",
       "      <td>This is an okay gift box, only if you like med...</td>\n",
       "      <td>0</td>\n",
       "      <td>This is an okay gift box, only if you like med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5440</th>\n",
       "      <td>negative</td>\n",
       "      <td>18/12/19</td>\n",
       "      <td>It looks llike I just walked into a raw deal. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>It looks llike I just walked into a raw deal. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5441</th>\n",
       "      <td>negative</td>\n",
       "      <td>19/1/20</td>\n",
       "      <td>Thank god that i tasted the metal before i swa...</td>\n",
       "      <td>0</td>\n",
       "      <td>Thank god that i tasted the metal before i swa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5442</th>\n",
       "      <td>negative</td>\n",
       "      <td>13/9/20</td>\n",
       "      <td>This product was very good when I began buying...</td>\n",
       "      <td>0</td>\n",
       "      <td>This product was very good when I began buying...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5443</th>\n",
       "      <td>negative</td>\n",
       "      <td>10/7/20</td>\n",
       "      <td>Once again, Paragon has disappointed with this...</td>\n",
       "      <td>0</td>\n",
       "      <td>Once again, Paragon has disappointed with this...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5444 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentiment      Time                                               Text  \\\n",
       "0     positive   18/6/21  This is a very healthy dog food. Good for thei...   \n",
       "1     positive    7/7/21  I've been very pleased with the Natural Balanc...   \n",
       "2     positive   18/6/21  Before I was educated about feline nutrition, ...   \n",
       "3     positive    7/7/21  My holistic vet recommended this, along with a...   \n",
       "4     positive    1/7/21  I bought this coffee because its much cheaper ...   \n",
       "...        ...       ...                                                ...   \n",
       "5439  negative   26/2/21  This is an okay gift box, only if you like med...   \n",
       "5440  negative  18/12/19  It looks llike I just walked into a raw deal. ...   \n",
       "5441  negative   19/1/20  Thank god that i tasted the metal before i swa...   \n",
       "5442  negative   13/9/20  This product was very good when I began buying...   \n",
       "5443  negative   10/7/20  Once again, Paragon has disappointed with this...   \n",
       "\n",
       "      sentiment_label                               VADER_processed_Text  \n",
       "0                   1  This is a very healthy dog food. Good for thei...  \n",
       "1                   1  I've been very pleased with the Natural Balanc...  \n",
       "2                   1  Before I was educated about feline nutrition, ...  \n",
       "3                   1  My holistic vet recommended this, along with a...  \n",
       "4                   1  I bought this coffee because its much cheaper ...  \n",
       "...               ...                                                ...  \n",
       "5439                0  This is an okay gift box, only if you like med...  \n",
       "5440                0  It looks llike I just walked into a raw deal. ...  \n",
       "5441                0  Thank god that i tasted the metal before i swa...  \n",
       "5442                0  This product was very good when I began buying...  \n",
       "5443                0  Once again, Paragon has disappointed with this...  \n",
       "\n",
       "[5444 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e72c9f9-6d74-433e-898d-361ccb374dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>VADER_processed_Text</th>\n",
       "      <th>VADER_dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>18/6/21</td>\n",
       "      <td>This is a very healthy dog food. Good for thei...</td>\n",
       "      <td>1</td>\n",
       "      <td>This is a very healthy dog food. Good for thei...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.705, 'pos': 0.295, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>7/7/21</td>\n",
       "      <td>I've been very pleased with the Natural Balanc...</td>\n",
       "      <td>1</td>\n",
       "      <td>I've been very pleased with the Natural Balanc...</td>\n",
       "      <td>{'neg': 0.031, 'neu': 0.732, 'pos': 0.237, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>18/6/21</td>\n",
       "      <td>Before I was educated about feline nutrition, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Before I was educated about feline nutrition, ...</td>\n",
       "      <td>{'neg': 0.017, 'neu': 0.795, 'pos': 0.188, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>7/7/21</td>\n",
       "      <td>My holistic vet recommended this, along with a...</td>\n",
       "      <td>1</td>\n",
       "      <td>My holistic vet recommended this, along with a...</td>\n",
       "      <td>{'neg': 0.079, 'neu': 0.67, 'pos': 0.252, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>1/7/21</td>\n",
       "      <td>I bought this coffee because its much cheaper ...</td>\n",
       "      <td>1</td>\n",
       "      <td>I bought this coffee because its much cheaper ...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.843, 'pos': 0.157, 'comp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment     Time                                               Text  \\\n",
       "0  positive  18/6/21  This is a very healthy dog food. Good for thei...   \n",
       "1  positive   7/7/21  I've been very pleased with the Natural Balanc...   \n",
       "2  positive  18/6/21  Before I was educated about feline nutrition, ...   \n",
       "3  positive   7/7/21  My holistic vet recommended this, along with a...   \n",
       "4  positive   1/7/21  I bought this coffee because its much cheaper ...   \n",
       "\n",
       "   sentiment_label                               VADER_processed_Text  \\\n",
       "0                1  This is a very healthy dog food. Good for thei...   \n",
       "1                1  I've been very pleased with the Natural Balanc...   \n",
       "2                1  Before I was educated about feline nutrition, ...   \n",
       "3                1  My holistic vet recommended this, along with a...   \n",
       "4                1  I bought this coffee because its much cheaper ...   \n",
       "\n",
       "                                          VADER_dict  \n",
       "0  {'neg': 0.0, 'neu': 0.705, 'pos': 0.295, 'comp...  \n",
       "1  {'neg': 0.031, 'neu': 0.732, 'pos': 0.237, 'co...  \n",
       "2  {'neg': 0.017, 'neu': 0.795, 'pos': 0.188, 'co...  \n",
       "3  {'neg': 0.079, 'neu': 0.67, 'pos': 0.252, 'com...  \n",
       "4  {'neg': 0.0, 'neu': 0.843, 'pos': 0.157, 'comp...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SIA = SentimentIntensityAnalyzer()\n",
    "reviews_csv['VADER_dict'] = reviews_csv['VADER_processed_Text'].apply(lambda text: SIA.polarity_scores(text))\n",
    "\n",
    "reviews_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee170f2f-7677-40a8-9546-9be04e0eea3e",
   "metadata": {},
   "source": [
    "##### The compound score is the sum of positive, negative & neutral scores which is then normalized between -1(most extreme negative) and +1 (most extreme positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "918a2f75-cf4b-41d0-b816-875b6afe8b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>VADER_processed_Text</th>\n",
       "      <th>VADER_dict</th>\n",
       "      <th>VADER_score</th>\n",
       "      <th>VADER_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>18/6/21</td>\n",
       "      <td>This is a very healthy dog food. Good for thei...</td>\n",
       "      <td>1</td>\n",
       "      <td>This is a very healthy dog food. Good for thei...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.705, 'pos': 0.295, 'comp...</td>\n",
       "      <td>0.8313</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>7/7/21</td>\n",
       "      <td>I've been very pleased with the Natural Balanc...</td>\n",
       "      <td>1</td>\n",
       "      <td>I've been very pleased with the Natural Balanc...</td>\n",
       "      <td>{'neg': 0.031, 'neu': 0.732, 'pos': 0.237, 'co...</td>\n",
       "      <td>0.9273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>18/6/21</td>\n",
       "      <td>Before I was educated about feline nutrition, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Before I was educated about feline nutrition, ...</td>\n",
       "      <td>{'neg': 0.017, 'neu': 0.795, 'pos': 0.188, 'co...</td>\n",
       "      <td>0.9769</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>7/7/21</td>\n",
       "      <td>My holistic vet recommended this, along with a...</td>\n",
       "      <td>1</td>\n",
       "      <td>My holistic vet recommended this, along with a...</td>\n",
       "      <td>{'neg': 0.079, 'neu': 0.67, 'pos': 0.252, 'com...</td>\n",
       "      <td>0.9678</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>1/7/21</td>\n",
       "      <td>I bought this coffee because its much cheaper ...</td>\n",
       "      <td>1</td>\n",
       "      <td>I bought this coffee because its much cheaper ...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.843, 'pos': 0.157, 'comp...</td>\n",
       "      <td>0.8868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment     Time                                               Text  \\\n",
       "0  positive  18/6/21  This is a very healthy dog food. Good for thei...   \n",
       "1  positive   7/7/21  I've been very pleased with the Natural Balanc...   \n",
       "2  positive  18/6/21  Before I was educated about feline nutrition, ...   \n",
       "3  positive   7/7/21  My holistic vet recommended this, along with a...   \n",
       "4  positive   1/7/21  I bought this coffee because its much cheaper ...   \n",
       "\n",
       "   sentiment_label                               VADER_processed_Text  \\\n",
       "0                1  This is a very healthy dog food. Good for thei...   \n",
       "1                1  I've been very pleased with the Natural Balanc...   \n",
       "2                1  Before I was educated about feline nutrition, ...   \n",
       "3                1  My holistic vet recommended this, along with a...   \n",
       "4                1  I bought this coffee because its much cheaper ...   \n",
       "\n",
       "                                          VADER_dict  VADER_score  VADER_label  \n",
       "0  {'neg': 0.0, 'neu': 0.705, 'pos': 0.295, 'comp...       0.8313            1  \n",
       "1  {'neg': 0.031, 'neu': 0.732, 'pos': 0.237, 'co...       0.9273            1  \n",
       "2  {'neg': 0.017, 'neu': 0.795, 'pos': 0.188, 'co...       0.9769            1  \n",
       "3  {'neg': 0.079, 'neu': 0.67, 'pos': 0.252, 'com...       0.9678            1  \n",
       "4  {'neg': 0.0, 'neu': 0.843, 'pos': 0.157, 'comp...       0.8868            1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the score to appropriate label {0,1}\n",
    "reviews_csv['VADER_score'] = reviews_csv['VADER_dict'].apply(lambda sent_dict: sent_dict['compound'])\n",
    "reviews_csv['VADER_label'] = 0\n",
    "\n",
    "# If compound > 0 -> 1 else compund < 0 -> 0\n",
    "reviews_csv.loc[reviews_csv['VADER_score'] > 0, 'VADER_label'] = 1\n",
    "reviews_csv.loc[reviews_csv['VADER_score'] < 0, 'VADER_label'] = 0\n",
    "reviews_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ce1aea7-0230-45da-9614-a481f2703be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.8872441309124551\n",
      "PR_AUC score:  0.826482206621329\n",
      "ROC_AUC score:  0.6997636186872852\n",
      "Accuracy:  0.8209037472446731\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 score: \", f1_score(reviews_csv['sentiment_label'], reviews_csv['VADER_label']))\n",
    "print(\"PR_AUC score: \", average_precision_score(reviews_csv['sentiment_label'], reviews_csv['VADER_label']))\n",
    "print(\"ROC_AUC score: \", roc_auc_score(reviews_csv['sentiment_label'], reviews_csv['VADER_label']))\n",
    "print(\"Accuracy: \", accuracy_score(reviews_csv['sentiment_label'], reviews_csv['VADER_label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e01042-5dd3-47c9-9ac8-2a978d2b287e",
   "metadata": {},
   "source": [
    "#### Let's try to improve scores - \n",
    "Instead of calculating polarity score on entire review, we calculate it for each sentatence in text and average it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78a433ce-12e9-4d95-a325-2fcb227104be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Average Vader score: by avergaing score for each sentence in input text\n",
    "def avg_pol_score(text):\n",
    "    compound_list = []\n",
    "    for sent in text:\n",
    "        dict_ = SIA.polarity_scores(sent)\n",
    "        compound_list.append(dict_['compound'])\n",
    "\n",
    "    return np.average(compound_list)\n",
    "\n",
    "# Tokenize Text\n",
    "reviews_csv['VADER_processed_Text'] = reviews_csv['VADER_processed_Text'].apply(lambda para: tokenize.sent_tokenize(para))\n",
    "avg_VADER_score = reviews_csv['VADER_processed_Text'].apply(lambda text: avg_pol_score(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6425937e-a2b2-44f4-b44e-f2c74ffcbe71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>VADER_processed_Text</th>\n",
       "      <th>VADER_dict</th>\n",
       "      <th>VADER_score</th>\n",
       "      <th>VADER_label</th>\n",
       "      <th>avg_VADER_score</th>\n",
       "      <th>avg_VADER_score_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>18/6/21</td>\n",
       "      <td>This is a very healthy dog food. Good for thei...</td>\n",
       "      <td>1</td>\n",
       "      <td>[This is a very healthy dog food., Good for th...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.705, 'pos': 0.295, 'comp...</td>\n",
       "      <td>0.8313</td>\n",
       "      <td>1</td>\n",
       "      <td>0.334600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>7/7/21</td>\n",
       "      <td>I've been very pleased with the Natural Balanc...</td>\n",
       "      <td>1</td>\n",
       "      <td>[I've been very pleased with the Natural Balan...</td>\n",
       "      <td>{'neg': 0.031, 'neu': 0.732, 'pos': 0.237, 'co...</td>\n",
       "      <td>0.9273</td>\n",
       "      <td>1</td>\n",
       "      <td>0.459625</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>18/6/21</td>\n",
       "      <td>Before I was educated about feline nutrition, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Before I was educated about feline nutrition,...</td>\n",
       "      <td>{'neg': 0.017, 'neu': 0.795, 'pos': 0.188, 'co...</td>\n",
       "      <td>0.9769</td>\n",
       "      <td>1</td>\n",
       "      <td>0.277511</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>7/7/21</td>\n",
       "      <td>My holistic vet recommended this, along with a...</td>\n",
       "      <td>1</td>\n",
       "      <td>[My holistic vet recommended this, along with ...</td>\n",
       "      <td>{'neg': 0.079, 'neu': 0.67, 'pos': 0.252, 'com...</td>\n",
       "      <td>0.9678</td>\n",
       "      <td>1</td>\n",
       "      <td>0.261840</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>1/7/21</td>\n",
       "      <td>I bought this coffee because its much cheaper ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[I bought this coffee because its much cheaper...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.843, 'pos': 0.157, 'comp...</td>\n",
       "      <td>0.8868</td>\n",
       "      <td>1</td>\n",
       "      <td>0.274083</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment     Time                                               Text  \\\n",
       "0  positive  18/6/21  This is a very healthy dog food. Good for thei...   \n",
       "1  positive   7/7/21  I've been very pleased with the Natural Balanc...   \n",
       "2  positive  18/6/21  Before I was educated about feline nutrition, ...   \n",
       "3  positive   7/7/21  My holistic vet recommended this, along with a...   \n",
       "4  positive   1/7/21  I bought this coffee because its much cheaper ...   \n",
       "\n",
       "   sentiment_label                               VADER_processed_Text  \\\n",
       "0                1  [This is a very healthy dog food., Good for th...   \n",
       "1                1  [I've been very pleased with the Natural Balan...   \n",
       "2                1  [Before I was educated about feline nutrition,...   \n",
       "3                1  [My holistic vet recommended this, along with ...   \n",
       "4                1  [I bought this coffee because its much cheaper...   \n",
       "\n",
       "                                          VADER_dict  VADER_score  \\\n",
       "0  {'neg': 0.0, 'neu': 0.705, 'pos': 0.295, 'comp...       0.8313   \n",
       "1  {'neg': 0.031, 'neu': 0.732, 'pos': 0.237, 'co...       0.9273   \n",
       "2  {'neg': 0.017, 'neu': 0.795, 'pos': 0.188, 'co...       0.9769   \n",
       "3  {'neg': 0.079, 'neu': 0.67, 'pos': 0.252, 'com...       0.9678   \n",
       "4  {'neg': 0.0, 'neu': 0.843, 'pos': 0.157, 'comp...       0.8868   \n",
       "\n",
       "   VADER_label  avg_VADER_score  avg_VADER_score_label  \n",
       "0            1         0.334600                      1  \n",
       "1            1         0.459625                      1  \n",
       "2            1         0.277511                      1  \n",
       "3            1         0.261840                      1  \n",
       "4            1         0.274083                      1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_csv['avg_VADER_score'] = pd.DataFrame(avg_VADER_score)\n",
    "reviews_csv['avg_VADER_score_label'] = 0\n",
    "reviews_csv.loc[reviews_csv['avg_VADER_score'] > 0, 'avg_VADER_score_label'] = 1\n",
    "reviews_csv.loc[reviews_csv['avg_VADER_score'] < 0, 'avg_VADER_score_label'] = 0\n",
    "reviews_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a498a66-d5fe-4a3b-8688-b91d114f46d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.8907621247113163\n",
      "PR_AUC score:  0.8290624565744616\n",
      "ROC_AUC score:  0.7051979320583601\n",
      "Accuracy:  0.8262307127112417\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 score: \", f1_score(reviews_csv['sentiment_label'], reviews_csv['avg_VADER_score_label']))\n",
    "print(\"PR_AUC score: \", average_precision_score(reviews_csv['sentiment_label'], reviews_csv['avg_VADER_score_label']))\n",
    "print(\"ROC_AUC score: \", roc_auc_score(reviews_csv['sentiment_label'], reviews_csv['avg_VADER_score_label']))\n",
    "print(\"Accuracy: \", accuracy_score(reviews_csv['sentiment_label'], reviews_csv['avg_VADER_score_label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0c6e79-b902-4210-a791-e64cb04b87c1",
   "metadata": {},
   "source": [
    "#### Drawback of VADER:\n",
    "The main drawback with the rule-based approach for sentiment analysis is that the method only cares about individual words and completely ignores the context in which it is used. \n",
    "\n",
    "For example, “the party was savage” will be negative when considered by any token-based algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246257ea-2508-4127-a7d4-15dc166dc347",
   "metadata": {},
   "source": [
    "## Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26a71aaf-91fd-4e87-b1e2-f395aa094845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nnerella/Documents/GitHub/DSA4263-Voice-of-Customer-VOC-analysis/voc_venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-22 18:22:18,713 https://nlp.informatik.hu-berlin.de/resources/models/sentiment-curated-distilbert/sentiment-en-mix-distillbert_4.pt not found in cache, downloading to /var/folders/vs/ybxh3hyn7_7bpjzkl7xwj4700000gn/T/tmpsa_klk_x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 253M/253M [02:15<00:00, 1.96MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-22 18:24:35,286 copying /var/folders/vs/ybxh3hyn7_7bpjzkl7xwj4700000gn/T/tmpsa_klk_x to cache at /Users/nnerella/.flair/models/sentiment-en-mix-distillbert_4.pt\n",
      "2023-03-22 18:24:35,365 removing temp file /var/folders/vs/ybxh3hyn7_7bpjzkl7xwj4700000gn/T/tmpsa_klk_x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading: 100%|███████████████████████████| 28.0/28.0 [00:00<00:00, 5.51kB/s]\n",
      "Downloading: 100%|██████████████████████████████| 483/483 [00:00<00:00, 109kB/s]\n",
      "Downloading: 100%|███████████████████████████| 232k/232k [00:36<00:00, 6.36kB/s]\n",
      "Downloading: 100%|████████████████████████████| 466k/466k [00:01<00:00, 465kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence above is:  ['Sentence[5]: \"The food was great!\"'/'POSITIVE' (0.9961)]\n"
     ]
    }
   ],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "classifier = TextClassifier.load('en-sentiment')\n",
    "sentence = Sentence('The food was great!')\n",
    "classifier.predict(sentence)\n",
    "\n",
    "# print sentence with predicted labels\n",
    "print('Sentence above is: ', sentence.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c04dc359-4574-49a9-b51d-6b2f5b3e7651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sentence[5]: \"The food was great!\"'/'POSITIVE' (0.9961)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80a03d13-28c0-406a-ad99-fd544c05d938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'value': 'POSITIVE', 'confidence': 0.9961493015289307}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.to_dict()['all labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8e40832f-9b9e-47a6-8690-e09c28d36fb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[43mreviews_csv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mSentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m flair_predictions \u001b[38;5;241m=\u001b[39m reviews_csv\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: classifier\u001b[38;5;241m.\u001b[39mpredict(row))\n",
      "File \u001b[0;32m~/Documents/GitHub/DSA4263-Voice-of-Customer-VOC-analysis/voc_venv/lib/python3.9/site-packages/pandas/core/frame.py:9568\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9557\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9559\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m   9560\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   9561\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9566\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   9567\u001b[0m )\n\u001b[0;32m-> 9568\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/DSA4263-Voice-of-Customer-VOC-analysis/voc_venv/lib/python3.9/site-packages/pandas/core/apply.py:764\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[0;32m--> 764\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/DSA4263-Voice-of-Customer-VOC-analysis/voc_venv/lib/python3.9/site-packages/pandas/core/apply.py:891\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 891\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/Documents/GitHub/DSA4263-Voice-of-Customer-VOC-analysis/voc_venv/lib/python3.9/site-packages/pandas/core/apply.py:907\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m    906\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 907\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    909\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    910\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    911\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sentences \u001b[38;5;241m=\u001b[39m reviews_csv\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[43mSentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      2\u001b[0m flair_predictions \u001b[38;5;241m=\u001b[39m reviews_csv\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: classifier\u001b[38;5;241m.\u001b[39mpredict(row))\n",
      "File \u001b[0;32m~/Documents/GitHub/DSA4263-Voice-of-Customer-VOC-analysis/voc_venv/lib/python3.9/site-packages/flair/data.py:710\u001b[0m, in \u001b[0;36mSentence.__init__\u001b[0;34m(self, text, use_tokenizer, language_code, start_position)\u001b[0m\n\u001b[1;32m    708\u001b[0m     text \u001b[38;5;241m=\u001b[39m Sentence\u001b[38;5;241m.\u001b[39m_handle_problem_characters(text)\n\u001b[1;32m    709\u001b[0m     words \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n\u001b[0;32m--> 710\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m text \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], Token):\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m text:\n\u001b[1;32m    712\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_token(t)\n",
      "File \u001b[0;32m~/Documents/GitHub/DSA4263-Voice-of-Customer-VOC-analysis/voc_venv/lib/python3.9/site-packages/pandas/core/generic.py:1527\u001b[0m, in \u001b[0;36mNDFrame.__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__nonzero__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1528\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe truth value of a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is ambiguous. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1529\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse a.empty, a.bool(), a.item(), a.any() or a.all().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1530\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "sentences = reviews_csv.apply(lambda row: Sentence(row))\n",
    "flair_predictions = reviews_csv.apply(lambda row: classifier.predict(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1f8d6a-185b-4c0d-830e-e81199249447",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea61b5ae-6680-49ca-942f-e8482c98dcb1",
   "metadata": {},
   "source": [
    "Refernce: https://www.section.io/engineering-education/sentiment-analysis-with-spacy-and-scikit-learn/\n",
    "\n",
    "Using Spacy tokeiser and linear SVC (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e3f516-cca6-46cb-b9a6-72341026cf28",
   "metadata": {},
   "source": [
    "### Results (vectorizer)\n",
    "- F1 score:  0.9232273838630808\n",
    "\n",
    "- PR_AUC score:  0.9089266836550011\n",
    "\n",
    "- ROC_AUC score:  0.8446448118914558\n",
    "\n",
    "- Accuracy : 0.884643644379133"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58ce6ab-1379-4482-a80c-4684ab6e9b8b",
   "metadata": {},
   "source": [
    "### Results (tfvectorizer)\n",
    "- F1 score:  0.9364161849710984\n",
    "\n",
    "- PR_AUC score:  0.9131134755677366\n",
    "\n",
    "- ROC_AUC score:  0.8539450941983062\n",
    "\n",
    "- Accuracy : 0.9030124908155768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46445cb0-7f36-4d8f-b911-677892bcd7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voc_venv",
   "language": "python",
   "name": "voc_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
