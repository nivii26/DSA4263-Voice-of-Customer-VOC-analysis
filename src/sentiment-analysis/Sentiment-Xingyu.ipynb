{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbfbf1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import sklearn.metrics\n",
    "import nltk\n",
    "import nlpaug.augmenter.word as naw\n",
    "import numpy as np\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e76592b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2b700947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from scipy.stats import uniform\n",
    "from scipy import interp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f4bd1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load the data from the csv file\n",
    "train_data = pd.read_csv(\"reviews.csv\")\n",
    "# train_label = train_data[\"Sentiment\"]\n",
    "original_data = pd.DataFrame(train_data[\"Text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d81f173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    # remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-z]', ' ', text)\n",
    "    # tokenize the text into words\n",
    "    tokens = word_tokenize(text)\n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    tokens= ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# remove the html symbol\n",
    "def remove_html(text):\n",
    "    regex = r\"<[^>]+>\"\n",
    "    text_new = re.sub(regex, \" \", text)\n",
    "    return text_new\n",
    "\n",
    "# apply the preprocessing function to the text data\n",
    "train_data['Text'] = train_data['Text'].apply(remove_html)\n",
    "train_data['Text'] = train_data['Text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3afbad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define an NLPAug data augmentation function\n",
    "def augment_text(text):\n",
    "    # define an augmentation method\n",
    "    aug = naw.SynonymAug(aug_src='wordnet', lang='eng')\n",
    "    # apply the augmentation method to the text\n",
    "    augmented_text = aug.augment(text)\n",
    "    return augmented_text\n",
    "\n",
    "# apply the augmentation function to the preprocessed text data\n",
    "train_data['Text'] = train_data['Text'].apply(augment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "582e8667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# over sampling\n",
    "ros = RandomOverSampler(sampling_strategy='minority')\n",
    "\n",
    "X = train_data['Text'].values.reshape(-1, 1)\n",
    "y = train_data['Sentiment']\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "train_data = pd.DataFrame({'Text': X_resampled.ravel(), 'Sentiment': y_resampled})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d4e3a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data to a new csv file\n",
    "train_data.to_csv(\"oversampling_reviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0ad767",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dddaad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a Word2Vec model on the preprocessed text data\n",
    "word2vec_model = Word2Vec(train_data['Text'], min_count=1)\n",
    "\n",
    "# create a function to generate the word embedding vectors for each sentence\n",
    "def generate_word_embedding(sentence):\n",
    "    # initialize an empty array for the sentence vector\n",
    "    sentence_vector = []\n",
    "    # loop through each word in the sentence\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            # add the vector representation of the word to the sentence vector\n",
    "            word_vector = word2vec_model.wv[word]\n",
    "            sentence_vector.append(word_vector)\n",
    "        except KeyError:\n",
    "            # ignore words that are not in the vocabulary\n",
    "            pass\n",
    "    # take the mean of the word vectors to get the sentence vector\n",
    "    sentence_vector = np.mean(sentence_vector, axis=0)\n",
    "    return sentence_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cae1851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the generate_word_embedding() function to the preprocessed text data\n",
    "train_data['embedding'] = train_data['Text'].apply(generate_word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4182170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new DataFrame for the feature matrix\n",
    "embedding_size = word2vec_model.vector_size\n",
    "features_df = pd.DataFrame(train_data['embedding'].tolist(), columns=[f'embedding_{i}' for i in range(embedding_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f68399ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform PCA with n_components set to retain 98% of variance\n",
    "pca_emb = PCA(n_components=0.98)\n",
    "features_emb_pca = pca_emb.fit_transform(features_df)\n",
    "\n",
    "# create a new DataFrame for the PCA features\n",
    "pca_emb_cols = [f\"PC_emb{i+1}\" for i in range(features_emb_pca.shape[1])]\n",
    "pca_df_emb = pd.DataFrame(features_emb_pca, columns=pca_emb_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f61c32f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a TF-IDF vectorizer object\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# fit and transform the vectorizer on the preprocessed text data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(train_data['Text'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_features_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d889571",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## standardize the features\n",
    "#scaler = StandardScaler()\n",
    "#features_std = scaler.fit_transform(features)\n",
    "\n",
    "# perform PCA with n_components set to retain 95% of variance\n",
    "pca = PCA(n_components=0.95)\n",
    "features_tfidf_pca = pca.fit_transform(tfidf_features_df)\n",
    "\n",
    "# create a new DataFrame for the PCA features\n",
    "pca_tfidf_cols = [f\"PC_tfidf{i+1}\" for i in range(features_tfidf_pca.shape[1])]\n",
    "pca_df_tfidf = pd.DataFrame(features_tfidf_pca, columns=pca_tfidf_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba292c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the TF-IDF features to the feature matrix DataFrame\n",
    "features_df = pd.concat([pca_df_tfidf, pca_df_emb], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c78cb80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the number of characters, number of words, and number of capital characters as features\n",
    "features_df['num_characters'] = train_data['Text'].apply(lambda x: len(' '.join(x)))\n",
    "features_df['num_words'] = train_data['Text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c5e6f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the common features from the features.csv file\n",
    "features_df['num_sentences'] = original_data[\"Text\"].apply(lambda s: s.count('.'))\n",
    "features_df['num_question_marks'] = original_data[\"Text\"].apply(lambda s: s.count('?'))\n",
    "features_df['num_exclamation_marks'] = original_data[\"Text\"].apply(lambda s: s.count('!'))\n",
    "features_df['num_unique_words'] = train_data[\"Text\"].apply(lambda x: len(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0130108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the label column to the feature matrix DataFrame\n",
    "label = features_df.columns\n",
    "features_df['Sentiment'] = train_data['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03d06370",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# weight the negative sentiment samples by 1.5\n",
    "features_df.loc[features_df['Sentiment'] == 'negative',label] *= 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2017a940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC_tfidf1</th>\n",
       "      <th>PC_tfidf2</th>\n",
       "      <th>PC_tfidf3</th>\n",
       "      <th>PC_tfidf4</th>\n",
       "      <th>PC_tfidf5</th>\n",
       "      <th>PC_tfidf6</th>\n",
       "      <th>PC_tfidf7</th>\n",
       "      <th>PC_tfidf8</th>\n",
       "      <th>PC_tfidf9</th>\n",
       "      <th>PC_tfidf10</th>\n",
       "      <th>...</th>\n",
       "      <th>PC_emb96</th>\n",
       "      <th>PC_emb97</th>\n",
       "      <th>PC_emb98</th>\n",
       "      <th>num_characters</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.071812</td>\n",
       "      <td>0.036136</td>\n",
       "      <td>0.076162</td>\n",
       "      <td>0.058094</td>\n",
       "      <td>0.015693</td>\n",
       "      <td>-0.068980</td>\n",
       "      <td>-0.009392</td>\n",
       "      <td>0.050062</td>\n",
       "      <td>-0.086675</td>\n",
       "      <td>-0.005748</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007832</td>\n",
       "      <td>-0.004934</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.130083</td>\n",
       "      <td>0.064077</td>\n",
       "      <td>0.186264</td>\n",
       "      <td>0.178121</td>\n",
       "      <td>0.043476</td>\n",
       "      <td>-0.063714</td>\n",
       "      <td>0.012203</td>\n",
       "      <td>0.060614</td>\n",
       "      <td>-0.090439</td>\n",
       "      <td>0.010428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008681</td>\n",
       "      <td>-0.003294</td>\n",
       "      <td>-0.003709</td>\n",
       "      <td>232</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.090828</td>\n",
       "      <td>0.038738</td>\n",
       "      <td>0.086590</td>\n",
       "      <td>0.030063</td>\n",
       "      <td>0.018296</td>\n",
       "      <td>0.020041</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.236875</td>\n",
       "      <td>-0.103662</td>\n",
       "      <td>-0.137266</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002308</td>\n",
       "      <td>0.007702</td>\n",
       "      <td>-0.007528</td>\n",
       "      <td>541</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.032244</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>0.016679</td>\n",
       "      <td>-0.011835</td>\n",
       "      <td>-0.005652</td>\n",
       "      <td>-0.009966</td>\n",
       "      <td>0.012565</td>\n",
       "      <td>0.031322</td>\n",
       "      <td>-0.033928</td>\n",
       "      <td>-0.066652</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001196</td>\n",
       "      <td>-0.003016</td>\n",
       "      <td>0.003852</td>\n",
       "      <td>311</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.051115</td>\n",
       "      <td>-0.117562</td>\n",
       "      <td>0.050717</td>\n",
       "      <td>-0.013965</td>\n",
       "      <td>-0.013435</td>\n",
       "      <td>-0.031462</td>\n",
       "      <td>-0.059974</td>\n",
       "      <td>0.022282</td>\n",
       "      <td>0.005212</td>\n",
       "      <td>0.054587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007418</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.007445</td>\n",
       "      <td>216</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3444 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PC_tfidf1  PC_tfidf2  PC_tfidf3  PC_tfidf4  PC_tfidf5  PC_tfidf6  \\\n",
       "0  -0.071812   0.036136   0.076162   0.058094   0.015693  -0.068980   \n",
       "1  -0.130083   0.064077   0.186264   0.178121   0.043476  -0.063714   \n",
       "2  -0.090828   0.038738   0.086590   0.030063   0.018296   0.020041   \n",
       "3  -0.032244   0.003467   0.016679  -0.011835  -0.005652  -0.009966   \n",
       "4   0.051115  -0.117562   0.050717  -0.013965  -0.013435  -0.031462   \n",
       "\n",
       "   PC_tfidf7  PC_tfidf8  PC_tfidf9  PC_tfidf10  ...  PC_emb96  PC_emb97  \\\n",
       "0  -0.009392   0.050062  -0.086675   -0.005748  ... -0.007832 -0.004934   \n",
       "1   0.012203   0.060614  -0.090439    0.010428  ...  0.008681 -0.003294   \n",
       "2   0.000002   0.236875  -0.103662   -0.137266  ... -0.002308  0.007702   \n",
       "3   0.012565   0.031322  -0.033928   -0.066652  ... -0.001196 -0.003016   \n",
       "4  -0.059974   0.022282   0.005212    0.054587  ...  0.007418  0.000506   \n",
       "\n",
       "   PC_emb98  num_characters  num_words  num_sentences  num_question_marks  \\\n",
       "0  0.000907              98          1            4.0                 0.0   \n",
       "1 -0.003709             232          1            4.0                 0.0   \n",
       "2 -0.007528             541          1            9.0                 0.0   \n",
       "3  0.003852             311          1            3.0                 0.0   \n",
       "4  0.007445             216          1            4.0                 0.0   \n",
       "\n",
       "   num_exclamation_marks  num_unique_words  Sentiment  \n",
       "0                    0.0                 1   positive  \n",
       "1                    0.0                 1   positive  \n",
       "2                    0.0                 1   positive  \n",
       "3                    2.0                 1   positive  \n",
       "4                    1.0                 1   positive  \n",
       "\n",
       "[5 rows x 3444 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03d692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the feature matrix to a CSV file\n",
    "# pca_df_emb.to_csv(\"pca_df_emb.csv\", index=False)\n",
    "# pca_df_tfidf.to_csv(\"pca_df_tfidf.csv\", index=False)\n",
    "#features_df.to_csv(\"features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f116cd8e",
   "metadata": {},
   "source": [
    "## XGBOOST\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63f82948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "442c01f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8060,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df['tag'] = features_df['Sentiment'].map(dict(positive=1, negative=0))\n",
    "y=features_df['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a422b255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8060, 3437)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=features_df.iloc[: , :-8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "765c9be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b56077dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomSearchCV\n",
    "# define the parameters to tune\n",
    "param_dist = {\"learning_rate\": uniform(0, 2),\n",
    "              \"gamma\": uniform(1, 0.000001),\n",
    "              \"max_depth\": range(1,50),\n",
    "              \"n_estimators\": range(1,300),\n",
    "              \"min_child_weight\": range(1,10),\n",
    "              'n_jobs': range(1,5)}\n",
    "#instance of RandomSearchCV\n",
    "rs = RandomizedSearchCV(XGBClassifier(), param_distributions=param_dist, n_iter=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "90bad1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           callbacks=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None,\n",
       "                                           early_stopping_rounds=None,\n",
       "                                           enable_categorical=False,\n",
       "                                           eval_metric=None, gamma=None,\n",
       "                                           gpu_id=None, grow_policy=None,\n",
       "                                           importance_type=None,\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None, max_bin=None,\n",
       "                                           max_c...\n",
       "                                           predictor=None, random_state=None,\n",
       "                                           reg_alpha=None, reg_lambda=None, ...),\n",
       "                   n_iter=3,\n",
       "                   param_distributions={'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f95062c1370>,\n",
       "                                        'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f94c04d9940>,\n",
       "                                        'max_depth': range(1, 50),\n",
       "                                        'min_child_weight': range(1, 10),\n",
       "                                        'n_estimators': range(1, 300),\n",
       "                                        'n_jobs': range(1, 5)})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "444b9ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1612,)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds=rs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "92245039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy:  98.76 %\n",
      "0.9876998769987699\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00e49e5",
   "metadata": {},
   "source": [
    "## Ramdon Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a191ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
